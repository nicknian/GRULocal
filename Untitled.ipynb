{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import calendar\n",
    "import pickle\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "import os.path\n",
    "import time\n",
    "import sys\n",
    "# you need to install the trlib and change the path to the path you installed the trlib\n",
    "# trlib can be found in  https://github.com/felixlen/trlib\n",
    "sys.path.insert(0, '/home/knian/trlib-master/')\n",
    "import trlib\n",
    "from tensorflow.python import debug as tf_debug\n",
    "# tensorflow_forward_ad can be found in https://pypi.org/project/tensorflow_forward_ad/\n",
    "from tensorflow_forward_ad.second_order import hessian_vec_bk\n",
    "import gc\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_Model(object):\n",
    "    def __init__(self, inp_dim, inp_seq, inp_dim_y, state_dim, state_output):\n",
    "        # Define placeholders for sequential input, tabular input, and output\n",
    "        self.x_t = tf.placeholder(tf.float32, [inp_seq,None,inp_dim])\n",
    "        self.y = tf.placeholder(tf.float32, [None,inp_dim_y])\n",
    "        self.z = tf.placeholder(tf.float32, [None,3])\n",
    "        self.cs = tf.placeholder(tf.float32, [None])\n",
    "        self.cv = tf.placeholder(tf.float32, [None])\n",
    "        self.bs = tf.placeholder(tf.float32, [None])\n",
    "        # setting huber threshold as the placeholder if you want to set manual threshold you can set it.\n",
    "        # in paper, huber threshold is set to be the hedging error of bs hedging\n",
    "        self.threshold = tf.placeholder(tf.float32, shape=())\n",
    "        #setting the dropout placeholder and penalty for parameter norm which is used to regularized the network when it is large.\n",
    "        self.keep_prob = tf.placeholder(tf.float32, shape=())\n",
    "        self.penalty = tf.placeholder(tf.float32, shape=())\n",
    "        # create the large parameters vector\n",
    "        orthogonal=tf.orthogonal_initializer()\n",
    "        parameters = tf.Variable(tf.concat( [\n",
    "                                             #gated recurrent unit\n",
    "                                             tf.truncated_normal([state_dim * inp_dim*3, 1]),\n",
    "                                             tf.expand_dims(tf.reshape(orthogonal([state_dim , state_dim]),[-1]),1),\n",
    "                                             tf.expand_dims(tf.reshape(orthogonal([state_dim , state_dim]),[-1]),1),\n",
    "                                             tf.expand_dims(tf.reshape(orthogonal([state_dim , state_dim]),[-1]),1),\n",
    "                                             tf.truncated_normal([3*state_dim, 1]),\n",
    "                                             #NN decoder\n",
    "                                             tf.truncated_normal([state_dim*state_output, 1]),\n",
    "                                             tf.truncated_normal([state_output*(inp_dim_y+1), 1]),\n",
    "                                             tf.truncated_normal([state_output, 1]),\n",
    "                                             tf.truncated_normal([state_output, 1]),\n",
    "                                             #Final Gate to Determine Whether to use BS or not\n",
    "                                             tf.truncated_normal([state_output*state_dim, 1]),\n",
    "                                             tf.truncated_normal([state_output*(inp_dim_y+1), 1]),\n",
    "                                             tf.truncated_normal([state_output, 1]),\n",
    "                                             tf.truncated_normal([state_output, 1]),\n",
    "                                             # weighting\n",
    "                                             tf.ones([inp_dim_y+1, 1]),\n",
    "                                             tf.ones([inp_dim, 1]),\n",
    "                                             # MV model   \n",
    "                                             tf.truncated_normal([3, 1])],0))\n",
    "        \n",
    "        self.par=parameters\n",
    "        self.vec = tf.placeholder(tf.float32, shape=[parameters.get_shape()[0],1], name='vec')\n",
    "        self.update = tf.placeholder(tf.float32, shape=[parameters.get_shape()[0],1], name='update')\n",
    "        print('size of parameters', self.par.get_shape())\n",
    "        # placeholder for the initial hidden states\n",
    "        self.c=tf.placeholder(tf.float32, [None,state_dim])\n",
    "        self.h=tf.placeholder(tf.float32, [None,state_dim])\n",
    "        idx_from = 0     \n",
    "        #Define the GRU encoder weights\n",
    "        with tf.name_scope(\"GRU\") as scope:\n",
    "            self.U = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_dim*inp_dim*3, 1]), [3, inp_dim,state_dim])\n",
    "            idx_from = idx_from + state_dim*inp_dim*3\n",
    "            self.W = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_dim*state_dim*3, 1]), [3,state_dim, state_dim])\n",
    "            idx_from = idx_from + state_dim*state_dim*3\n",
    "            self.b = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_dim*3, 1]), [3,state_dim])\n",
    "            idx_from = idx_from + state_dim*3\n",
    "        \n",
    "        with tf.name_scope(\"Decoder\") as scope:\n",
    "            self.Uout = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_dim*state_output, 1]), [state_dim, state_output])\n",
    "            idx_from = idx_from + state_dim*state_output\n",
    "            self.Wout = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_output*(inp_dim_y+1), 1]), [inp_dim_y+1,state_output]) \n",
    "            idx_from = idx_from + state_output*(inp_dim_y+1)\n",
    "            self.bout = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_output, 1]), [state_output]) \n",
    "            idx_from = idx_from + state_output\n",
    "            self.Vout = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_output, 1]), [state_output,1])\n",
    "            idx_from = idx_from + state_output\n",
    "            \n",
    "        with tf.name_scope(\"FinalGates\") as scope:\n",
    "            self.Ugate = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_output*state_dim, 1]), [state_dim,state_output])\n",
    "            idx_from = idx_from + state_output*state_dim\n",
    "            self.Wgate = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_output*(inp_dim_y+1), 1]), [(inp_dim_y+1),state_output])\n",
    "            idx_from = idx_from + state_output*(inp_dim_y+1)\n",
    "            \n",
    "            self.bgate = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_output, 1]), [state_output])\n",
    "            idx_from = idx_from + state_output\n",
    "            self.Vgate = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[state_output, 1]), [state_output,1])\n",
    "            idx_from = idx_from + state_output\n",
    "       \n",
    "        \n",
    "        with tf.name_scope(\"weighting\") as scope:\n",
    "            self.FW = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[inp_dim_y+1, 1]), [inp_dim_y+1])\n",
    "            idx_from = idx_from + inp_dim_y+1\n",
    "            self.SW = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[inp_dim, 1]), [inp_dim])\n",
    "            idx_from = idx_from + inp_dim\n",
    "        with tf.name_scope(\"MV\") as scope:\n",
    "            self.MVa = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[1, 1]), [1])\n",
    "            idx_from = idx_from + 1\n",
    "            self.MVb = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[1, 1]), [1])\n",
    "            idx_from = idx_from + 1\n",
    "            self.MVc = tf.reshape(tf.slice(parameters, begin=[idx_from, 0], size=[1, 1]), [1])\n",
    "            idx_from = idx_from + 1\n",
    "       \n",
    "            \n",
    "\n",
    "        def forward_step_GRU(acc, x_i):\n",
    "            h_p = acc\n",
    "            x_hat=tf.matmul(x_i,self.U[0])\n",
    "            # apply DropOut to input layer\n",
    "            x_hat = tf.nn.dropout(x_hat, self.keep_prob) \n",
    "            h_hat=tf.matmul(h_p,self.W[0])\n",
    "            z_t = tf.nn.sigmoid(tf.add(x_hat+h_hat, self.b[0]))\n",
    "            x_hat=tf.matmul(x_i,self.U[1])\n",
    "            h_hat=tf.matmul(h_p,self.W[1])\n",
    "            x_hat = tf.nn.dropout(x_hat, self.keep_prob) \n",
    "            r_t = tf.nn.sigmoid(tf.add(x_hat + h_hat, self.b[1]))\n",
    "            x_hat=tf.matmul(x_i,self.U[2])\n",
    "            h_c=tf.multiply(r_t,h_p)\n",
    "            h_hat=tf.matmul(h_c,self.W[2])\n",
    "            x_hat = tf.nn.dropout(x_hat, self.keep_prob) \n",
    "            o_t = tf.nn.tanh(tf.add(x_hat + h_hat, self.b[2]))\n",
    "            h= tf.multiply(1-z_t,h_p) + tf.multiply(z_t,o_t)\n",
    "            return h\n",
    "        \n",
    "        \n",
    "        # prepare the local input ans weight the local input\n",
    "        MV=tf.multiply(self.MVa,self.z[:,0])+tf.multiply(self.MVb,self.z[:,1])+tf.multiply(self.MVc,self.z[:,2])+self.bs\n",
    "        y_W=tf.nn.softmax(self.FW)\n",
    "        w_y=np.multiply(self.y,y_W[0:-1])\n",
    "        WMV=np.multiply(MV,y_W[-1])\n",
    "        y_concat = tf.concat([tf.expand_dims(WMV,1),w_y], axis = 1)\n",
    "        #weight the sequential input\n",
    "        S_W=tf.nn.softmax(self.SW)\n",
    "        W_feature_k=[]\n",
    "        for k in range(inp_dim):\n",
    "            W_feature_k.append(tf.expand_dims(tf.multiply(self.x_t[:,:,k],S_W[k]),2))\n",
    "        w_x_t=tf.concat(W_feature_k,axis=2)\n",
    "        print('w_x_t:',w_x_t.get_shape())\n",
    "        \n",
    "        # Step through the sequence of input words, each one at a time using weighted input X\n",
    "        h_init = self.h\n",
    "        hiddenlist=[]\n",
    "        hiddenlist.append(h_init)\n",
    "        for i in range(inp_seq):\n",
    "            h = forward_step_GRU(hiddenlist[-1],w_x_t[i,:,:])\n",
    "            hiddenlist.append(h)\n",
    "        # the hidden states from last step is used in decoder\n",
    "        hE=hiddenlist[-1]\n",
    "        \n",
    "\n",
    "\n",
    "        hE_hat=tf.matmul(hE,self.Ugate)\n",
    "        y_hat=tf.matmul(y_concat,self.Wgate)\n",
    "        Gate_hat = tf.nn.dropout(hE_hat, self.keep_prob)+tf.nn.dropout(y_hat, self.keep_prob)+self.bgate\n",
    "        GateV=tf.nn.sigmoid(tf.matmul(tf.tanh(Gate_hat),self.Vgate))\n",
    "        y_hat=tf.matmul(y_concat,self.Wout)\n",
    "        hE_hat=tf.matmul(hE,self.Uout)\n",
    "        y_hat = tf.nn.dropout(y_hat, self.keep_prob)\n",
    "        hE_hat = tf.nn.dropout(hE_hat, self.keep_prob)\n",
    "        finalY=y_hat+hE_hat+self.bout\n",
    "        predNN=tf.matmul(tf.tanh(finalY),self.Vout)\n",
    "        \n",
    "        \n",
    "        pred=tf.nn.sigmoid(predNN)\n",
    "        pred=tf.multiply(GateV,tf.expand_dims(self.bs,1))+tf.multiply(1-GateV,pred)\n",
    "        \n",
    "        pred=tf.squeeze(pred,axis=[-1])\n",
    "        self.pred=pred\n",
    "        self.y_concat=y_concat\n",
    "        self.hE=hE\n",
    "        self.y_W=y_W\n",
    "        self.S_W=S_W\n",
    "        self.hedge_Loss=tf.subtract(tf.multiply(pred,self.cs),self.cv)\n",
    "        self.hedge_Loss_bs=tf.subtract(tf.multiply(self.bs,self.cs),self.cv)\n",
    "        cost = tf.square(self.hedge_Loss)\n",
    "        costabs = tf.abs(self.hedge_Loss)\n",
    "        \n",
    "        \n",
    "        threshold=self.threshold\n",
    "        penalty=self.penalty\n",
    "        \n",
    "        penalized_par=tf.concat([tf.reshape(self.U,[-1]),tf.reshape(self.Uout,[-1]),\n",
    "         tf.reshape(self.Wout,[-1]),tf.reshape(self.Vout,[-1]),\n",
    "         tf.reshape(self.bout,[-1]),tf.reshape(self.Vgate,[-1]),tf.reshape(self.Ugate,[-1]),\n",
    "         tf.reshape(self.Wgate,[-1]),tf.reshape(self.bgate,[-1])],axis=0)\n",
    "        print('P_Par:', penalized_par.get_shape())\n",
    "        N=penalized_par.shape[0].value\n",
    "        penalized_norm=np.divide(tf.square(tf.norm(penalized_par)),N)\n",
    "          \n",
    "        \n",
    "        BScost = tf.square(self.hedge_Loss_bs)\n",
    "        \n",
    "        #alternative huber loss with threshold\n",
    "#         condition = tf.less(tf.abs(self.hedge_Loss), self.threshold)\n",
    "#         small_res = 0.5*tf.square(self.hedge_Loss)\n",
    "#        large_res = tf.multiply( self.threshold,tf.abs(self.hedge_Loss_bs))-0.5*tf.square(self.hedge_Loss_bs)\n",
    "#         costhuber= tf.where(condition, small_res, large_res)\n",
    "        \n",
    "        #huber loss\n",
    "        condition = tf.less(tf.abs(self.hedge_Loss), tf.abs(self.hedge_Loss_bs))\n",
    "        small_res = 0.5*tf.square(self.hedge_Loss)\n",
    "        large_res = tf.multiply(tf.abs(self.hedge_Loss),tf.abs(self.hedge_Loss_bs))-0.5*tf.square(self.hedge_Loss_bs)\n",
    "        costhuber= tf.where(condition, small_res, large_res)\n",
    "        \n",
    "        \n",
    "        #loss to push model to output bs initially\n",
    "        condition = tf.less(tf.abs(self.hedge_Loss), tf.abs(self.hedge_Loss_bs))\n",
    "        large_res = tf.abs(self.hedge_Loss_bs-self.hedge_Loss)\n",
    "        small_res = tf.abs(self.hedge_Loss_bs)\n",
    "        costnonC= tf.where(condition, small_res, large_res)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.total_losshuber = tf.reduce_mean(costhuber)+self.penalty*penalized_norm\n",
    "        self.total_lossnonC = tf.reduce_mean(costnonC)\n",
    "        costMV = tf.square(tf.subtract(tf.multiply(MV,self.cs),self.cv))\n",
    "        BScost = tf.square(self.hedge_Loss_bs)\n",
    "        self.total_loss = tf.reduce_sum(cost)\n",
    "        self.total_lossabs = tf.reduce_sum(costabs)\n",
    "        self.total_loss_BS = tf.reduce_sum(BScost)\n",
    "        self.total_lossmv = tf.reduce_sum(costMV)\n",
    "        self.radius=1.0\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.initial_global_step = self.global_step.assign(0)\n",
    "        self.learning_rate = tf.train.exponential_decay(0.01, self.global_step, 1000, 0.96, staircase=True)\n",
    "        self.learning_adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.total_losshuber, global_step=self.global_step)\n",
    "        self.learning_adamNC = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.total_lossnonC, global_step=self.global_step)\n",
    "        self.grad=tf.gradients(self.total_losshuber,parameters)[0]\n",
    "        self.gradNC=tf.gradients(self.total_lossnonC,parameters)[0]\n",
    "        self.Hv = hessian_vec_bk(self.total_losshuber, parameters, self.vec,grads=self.grad)\n",
    "        self.load_par = self.par.assign(self.update)\n",
    "        \n",
    "        \n",
    "    def TrustRegionUpdate(self,feedH,feedG,maxRadius):\n",
    "        t0 = time.time()\n",
    "        p,approxloss=self.TRSTrlib(self.radius,feedH,feedG)\n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n",
    "        #print('TRS takes:',total)\n",
    "        p=p.real\n",
    "        curloss=self.sess.run(self.total_losshuber, feed_dict=feedG)\n",
    "        curpar=self.sess.run(self.par,feed_dict=feedG)\n",
    "        trialpar=curpar+p\n",
    "        self.sess.run(self.load_par, feed_dict={self.update:trialpar})\n",
    "        trialloss=self.sess.run(self.total_losshuber, feed_dict=feedG)\n",
    "        rho=(trialloss-curloss)/(approxloss)\n",
    "        eta = 0.1 \n",
    "        mu = 0 \n",
    "        gamma0 = 0.5 \n",
    "        gamma1 = 0.9\n",
    "        gamma2=1.1\n",
    "        \n",
    "        pnorm=np.linalg.norm(p)\n",
    "        if rho<0:\n",
    "            gammaK = gamma0\n",
    "            self.radius = min(self.radius*gammaK,maxRadius)\n",
    "            #print('shrink radius and revert the changes because rho<=0')\n",
    "            self.sess.run(self.load_par, feed_dict={self.update:curpar})\n",
    "        else:\n",
    "            if rho<mu:\n",
    "                #print('shrink radius and revert the changes because rho<=mu')\n",
    "                self.sess.run(self.load_par, feed_dict={self.update:curpar})\n",
    "                gammaK = max(gamma0,gamma1*pnorm/self.radius)\n",
    "                self.radius = self.radius*gammaK\n",
    "            else:\n",
    "                #print('accept trials')          \n",
    "                if rho>=eta:\n",
    "                    #print('enlarge radius')\n",
    "                    gammaK=gamma2\n",
    "                    self.radius = min(self.radius*gammaK,maxRadius)\n",
    "\n",
    "    def TRSTrlib(self,radius,feedH,feedG):\n",
    "        def HVFun(v,feed):\n",
    "            feed[model.vec]=np.expand_dims(v,axis=1)\n",
    "            HV=self.sess.run(model.Hv,feed_dict=feed)\n",
    "            HV=np.squeeze(HV)\n",
    "            return HV\n",
    "        g=self.sess.run(self.grad,feed_dict=feedG)\n",
    "        g=np.squeeze(g)\n",
    "        HVOps= lambda v: HVFun(v,feedH)\n",
    "        H = LinearOperator((g.shape[0],g.shape[0]), matvec=HVOps, rmatvec=HVOps)\n",
    "        p, TR = trlib.trlib_solve(H, g, radius,verbose=0)\n",
    "        #print('finish solving sub')\n",
    "        return np.expand_dims(p,axis=1),TR['obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialized the model\n",
    "tf.reset_default_graph()\n",
    "model=Basic_Model(inp_dim=6, inp_seq=6, inp_dim_y=9, state_dim=5, state_output=5)\n",
    "# we are not allowed to share the data but here is the what you need to provide.\n",
    "# XMat: sequential input\n",
    "# YMat: local input\n",
    "# ZMat: The three dimension needed for Hull-White MV model\n",
    "# CSMat: the vector of changes of stock prices\n",
    "# CVMat: the vector of changes of option prices\n",
    "# h: initial state value for GRU encoder (usually set to be zero)\n",
    "# prepare the feed dict\n",
    "feedMat = { model.x_t:XMat,model.y:YMat,model.z:ZMat,model.cs:CSMat,model.cv:CVMat,model.bs:BSMat,model.h:h}\n",
    "feedVal = { model.x_t:XVal,model.y:YVal,model.z:ZVal,model.cs:CSVal,model.cv:CVVal,model.bs:BSVal,model.h:hv}\n",
    "feedTest = { model.x_t:XTest,model.y:YTest,model.z:ZTest,model.cs:CSTest,model.cv:CVTest,model.bs:BSTest,model.h:ht}\n",
    "# prepare the feed dict, in the paper, we do not regularized the model parameter since the model is small. \n",
    "# You will need regularization if your model is getting larger.\n",
    "feedMat[model.penalty]=0.0\n",
    "feedVal[model.penalty]=0.0\n",
    "feedTest[model.penalty]=0.0\n",
    "feedMat[model.keep_prob]=1.0\n",
    "feedVal[model.keep_prob]=1.0\n",
    "feedTest[model.keep_prob]=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the model session    \n",
    "model.sess=tf.train.MonitoredTrainingSession(\n",
    "                                        save_checkpoint_secs=None,\n",
    "                                        save_summaries_steps=None,\n",
    "                                        save_summaries_secs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using adam to pre-training the model, if converged, we skip the trust region update    \n",
    "model.sess.run(model.initial_global_step)\n",
    "ConvergeFlag=False    \n",
    "for i in range(5000):\n",
    "    model.sess.run([model.learning_adam],feed_dict = feedMat)\n",
    "    grad=model.sess.run(model.grad,feed_dict=feedMat)\n",
    "    loss,loss_bs= model.sess.run([model.total_loss,model.total_loss_BS],feed_dict = feedMat)\n",
    "    lossV,loss_bsV= model.sess.run([model.total_loss,model.total_loss_BS],feed_dict = feedVal)\n",
    "    lossT,loss_bsT= model.sess.run([model.total_loss,model.total_loss_BS],feed_dict = feedTest)\n",
    "    if i%10==0:\n",
    "        print('Adam Step',i,'Date',CurDate)\n",
    "        print('Train Gain',1-loss/loss_bs)\n",
    "        print('Val Gain',1-lossV/loss_bsV)\n",
    "        print('Test Gain',1-lossT/loss_bsT)\n",
    "        print('Grad Norm',np.linalg.norm(grad))\n",
    "    CurVal=1-lossV/loss_bsV\n",
    "    CurTest=1-lossT/loss_bsT\n",
    "\n",
    "    #update the best val error\n",
    "    if CurVal>OverAllBest:\n",
    "        Par= model.sess.run([model.par])\n",
    "        OverAllBestPred= model.sess.run([model.pred],feed_dict = feedTest)\n",
    "        OverAllBestPar=(Par,OverAllBestPred,BSTest,CSTest,CVTest)\n",
    "        OverAllBest=CurVal\n",
    "        BestTest=CurTest\n",
    "    #update the best test error\n",
    "    if CurTest>BestVal:\n",
    "        Par= model.sess.run([model.par])\n",
    "        BestPred= model.sess.run([model.pred],feed_dict = feedTest)\n",
    "        BestPar=(Par,BestPred,BSTest,CSTest,CVTest)\n",
    "        BestVal=CurTest\n",
    "    if np.linalg.norm(grad)<=0.001:\n",
    "        print('get a critical point, break iteration')\n",
    "        print('Adam Step',i,'Date',CurDate)\n",
    "        print('Train Gain',1-loss/loss_bs)\n",
    "        print('Val Gain',1-lossV/loss_bsV)\n",
    "        print('Test Gain',1-lossT/loss_bsT)\n",
    "        print('Grad Norm',np.linalg.norm(grad))\n",
    "        print('Current Best Val ', OverAllBest,'Best Test',BestTest)\n",
    "        print('Actual Best Val ', BestVal)\n",
    "        ConvergeFlag=True\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using adam to pre-training the model, if converged, we skip the trust region update    \n",
    "for i in range(MaxIter):\n",
    "    model.TrustRegionUpdate(feedMat,feedMat,1000000.0)  \n",
    "    print('Trust Region Step',i)\n",
    "    print('Train Gain',1-loss/loss_bs)\n",
    "    print('Val Gain',1-lossV/loss_bsV)\n",
    "    print('Test Gain',1-lossT/loss_bsT)\n",
    "    print('Grad Norm',np.linalg.norm(grad))\n",
    "    print('Radius',model.radius)\n",
    "    CurVal=1-lossV/loss_bsV    \n",
    "    CurTest=1-lossT/loss_bsT\n",
    "    if model.radius<=10**-5:\n",
    "        print('model radius too small reset to 1.0, break iteration')\n",
    "        print('Trust Region Step',i,'Date',CurDate)\n",
    "        print('Train Gain',1-loss/loss_bs)\n",
    "        print('Val Gain',1-lossV/loss_bsV)\n",
    "        print('Test Gain',1-lossT/loss_bsT)\n",
    "        print('Grad Norm',np.linalg.norm(grad))\n",
    "        print('Radius',model.radius)\n",
    "        break\n",
    "    if np.linalg.norm(grad)<=0.001:\n",
    "        print('get a critical point, break iteration')\n",
    "        print('Trust Region Step',i,'Date',CurDate)\n",
    "        print('Train Gain',1-loss/loss_bs)\n",
    "        print('Val Gain',1-lossV/loss_bsV)\n",
    "        print('Test Gain',1-lossT/loss_bsT)\n",
    "        print('Grad Norm',np.linalg.norm(grad))\n",
    "        print('Radius',model.radius)\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
